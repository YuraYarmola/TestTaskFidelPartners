# Кроки роботи з проєктом
1. **Дослідив способи отримання результатів від пошуковика**
   * Вибрав Serper.dev як основний API-провайдер для Google SERP. Чому? Легальний, стабільний, без симуляції браузера та без ризиків ToS. SerpAPI як fallback на випадок вичерпання квоти. *

2. **Підготовка оточення**

   * Створив `.env` із ключами (`SERPER_API_KEY`, опційно `SERPAPI_API_KEY`), регіоном (`GL/HL`), інтервалом запуску (`RUN_EVERY_SECONDS` або `SCHEDULE_CRON`).
   * Задав ключові слова в `config/keywords.txt` (1 запит на рядок).

3. **Запуск**

   * **Docker:** `docker compose up -d --build` → сервіс у режимі `serve` із вбудованим планувальником.
   * **Локально (без Docker):** `python app/serp_monitor.py run ...` (разовий прогін) або `serve` (демон).

4. **Перевірка артефактів**

   * База: `data/serp.db`.
   * Експорти: `data/exports/snapshot_YYYY-MM-DD.csv` і `domains_YYYY-MM-DD.csv`.
   * Логи: `docker compose logs -f serp-monitor`.

5. **Google Sheets (опційно)**

   * Увімкнув Sheets & Drive API, створив **Service Account**, додав його e-mail у **Editors** до таблиці.
   * В `.env` задав `PUSH_TO_SHEETS=1`, `GOOGLE_SHEETS_CREDENTIALS_JSON`, `SHEETS_KEY` (ID з URL).
   * Пуш іде **в уже існуючий Sheet** (через `open_by_key`), аркуші — `YYYY-MM-DD` та `domains`.

6. **Щоденний моніторинг**

   * Планувальник запускає збір SERP, фіксує позиції Топ-10/30, виявляє **нові домени**, паралельно збагачує їх (тип сайту + контакти), оновлює CSV/DB/Sheets.

# Чому саме така методика

* **Google SERP через API-провайдерів (Serper.dev + SerpAPI як fallback)**
  Обрав легальний та стабільний шлях без прямого скрейпінгу Google (ризики ToS, капчі, антиботи). Провайдери дають структурований JSON, прозорі ліміти та SLA. Fallback гарантує роботу при вичерпанні квоти або збоях.

* **Вбудований планувальник у сервісі**
  Дає автономність у Docker (не потрібен системний cron у контейнері), підтримує і **cron-рядок**, і **інтервал** — зручно при міграції між середовищами.

* **SQLite + CSV як перше сховище**
  Мінімальні залежності, швидкий старт, легко інспектувати. Для росту — безболісно переноситься на Postgres/BigQuery, а CSV лишаються універсальним звітом.

* **Паралельне збагачення (ThreadPool) + HTTP-пулінг і ретраї**
  Істотно пришвидшує обробку доменів, при цьому зберігає контрольовану «ввічливість» (throttle через `HTTP_DELAY`), ретраї — стабільність на нестійких сайтах.

* **Класифікація типу сайту: евристики + JSON-LD**
  Швидко, детерміновано, без тренування моделей: шукаю ознаки `schema.org` та патерни у title/H1/URL. Для MVP цього достатньо; за потреби легко підміняється на ML.

* **Витяг контактів — прості та надійні правила**
  `mailto:`, regex для e-mail, пошук сторінок `/contact`, `/about`, `impressum`, соцмережі — покриває більшість кейсів з низькою вартістю підтримки. Далі можна докрутити Hunter.io/WHOIS.

* **Docker-ізація + .env-конфіг**
  Відтворюваність, однаковість dev/prod, просте розгортання. Секрети та параметри — через змінні оточення, без хардкоду.

* **Google Sheets через service account**
  Вибрав **відкриття за ключем (ID)** і роботу лише з уже створеним файлом, щоб не упиратись у квоти Drive на створення/зберігання. Акцент на обов’язковому **Editor-доступі** для SA — інакше буде `PermissionError/403`.
